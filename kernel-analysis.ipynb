{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer blocks in Depth\n",
    "\n",
    "This notebook target to analyze the transformer blocks in depth, from pytorch code to the kernel level implementation, and discuss the details of the potential optimizations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Self attention layer\n",
    "\n",
    "### Vanilla self-attention layer in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SelfAtten(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=512) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qkv_proj = nn.Linear(dim, dim*3)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: [B, S, D]\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        # [B, S, S]\n",
    "        score = torch.matmul(q, k.transpose(-2, -1)) / (self.dim ** 0.5) \n",
    "        score = score.masked_fill(mask == 0, -1e3)\n",
    "        attn = score.softmax(dim=-1)\n",
    "        # [B, S, D]\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Kernels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch implementation of the above layer requires 10 kernels, which are listed below:\n",
    "\n",
    "4 GEMM kernels.\n",
    "1 Softmax kernel.\n",
    "And some elementwise for the mask compute.\n",
    "\n",
    "![kernels](./media/attention-torch-kernel-trace.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnx graph\n",
    "\n",
    "![onnx graph of attention layer](./media/attention.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT kernels\n",
    "\n",
    "After the onnx lowered to TRT, some elementwise kernels and softmax kernels are fused together, there are only 7 kernels in total.\n",
    "![kernels](./media/attention-trt-kernel.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Analyze of the fusions:\n",
    "\n",
    "- Bias of first GEMM, and 3 slices (tensor.chunk) are fused together into _myl_bb0_3_AddSliSliSli. The shape compute is done in host, no kernels needed.\n",
    "- Transpose of the QK^T gemm is fused with Matmul\n",
    "- Mask and softmax are fused together into _myl_bb0_2_*\n",
    "- The bias add after output projection gemm is not fused, which is not good.\n",
    "\n",
    "\n",
    "Some implementation can futher fuse the matmul-softmax-matmul into one kernel. In that case, only 4 kernels are needed.\n",
    "- 1 for QKV gemm, 1 for slice, 1 for fused attention, 1 for output projection gemm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadSelfAtten(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=512, head=8) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.head = head\n",
    "        self.qkv_proj = nn.Linear(dim, dim*3)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    # [B, S, D] -> [B, H, S, D/H]\n",
    "    def reshape(self, x):\n",
    "        return x.reshape(x.shape[0], -1, self.head, self.dim // self.head).transpose(1, 2)\n",
    "\n",
    "    def self_attn(self, q, k, v, mask):\n",
    "        # q, k, v: [B, H, S, D/H]\n",
    "        # out: [B, H, S, D/H]\n",
    "        score = torch.matmul(q, k.transpose(-2, -1)) / (self.dim ** 0.5) \n",
    "        score = score.masked_fill(mask == 0, -1e3)\n",
    "        attn = score.softmax(dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: [B, S, D]\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        batch = q.shape[0]\n",
    "        q, k, v = self.reshape(q), self.reshape(k), self.reshape(v)\n",
    "        out = self.self_attn(q, k, v, mask)\n",
    "\n",
    "        # merge heads: [B, H, S, D/H] -> [B, S, D]\n",
    "        out = out.transpose(1, 2).reshape(batch, -1, self.dim)\n",
    "\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnx graph of multi head attention\n",
    "![onnx graph of multi head attention](./media/multihead-attention-onnx.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare with the single head self-attention, multi-head attention has **addtional reshape and transpose** ops before the self-attention compute, split out one head dim and then moves the head dimension as batch, and do normal attention, and **then transpose->reshpae** back, so the onnx graph is more complicated.\n",
    "\n",
    "Following 9 TRT kernels are launched for the multi-head attention:\n",
    "\n",
    "![multi-head attention kernels](./media/multihead-attention-trt-kernel.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layer Norm\n",
    "\n",
    "In Pytorch LayerNorm has special op and dedicated kernel, but when exporting that to onnx, it's lowered to a sequence of ops.\n",
    "\n",
    "![layer norm](./media/layer-norm.png)\n",
    "\n",
    "TRT needs to recoginize this patten and fuse them into one kernel again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "norm = nn.LayerNorm(512) \n",
    "x = norm(torch.rand(1, 32, 512))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The onnx graph looks like this\n",
    "\n",
    "![onnx graph of layer norm](./media/layer-norm-onnx.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running LayerNorm only module by TRT, the reduce op can are break point which prevents the fusion.\n",
    "And the TRT graph looks like this:\n",
    "\n",
    "![TRT graph of layer norm](./media/layer-norm-trt.png)\n",
    "\n",
    "But when running the whole model, layer norm is fused into one kernel since TRT use special backend to handle the transformer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Whole Transformer Decoder Block\n",
    "\n",
    "Whole decoder block is a sequence of multihead-attention, layer norm and residual, feed forward, layer norm and residual."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![trt kernels of transformer decoder block](./media/decoder-attention-trt-kernel.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoel decoder block needs 13 kernels in TRT, addtional 4 kernels compared with the multi head attention.\n",
    "\n",
    "- 2 LayerNorm+Residual. LayerNorm and the residual are fused together, which is good. \n",
    "- 1 GELU kernel\n",
    "- 2 MLP gemm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Attention mask\n",
    "\n",
    "Mask is used to zero out scores in certain positions preventing them from being attended to.\n",
    "\n",
    "To prevent the current token from attending to the future tokens (since the GPT is trained and used in auto-regressive way, no token can know what future token is), a mask is applied to the attention layer, which is a matrix with 0s in the upper triangle (the right positions are masked out) and 1s in the lower triangle. \n",
    "\n",
    "The mask shape is `[S, S]` where `S` the seq length.\n",
    "\n",
    "![decoder mask](./media/decoder-mask.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When feeding the model in batches, the tokens of one sentence can be pre-padded to the same length, and a `[BATCH, PADDED LENGTH]` (abbrev as `[B, S]`) binary mask can be applied to prevent the attention layer from attending to the padded tokens, calling it padding mask in the following.\n",
    "\n",
    "Differences of two masks:\n",
    "1. the attention mask is a triu matrix of shape `[S, S]`, which is applied exact same way for all samples in the batch (broadcasted in batch dim).\n",
    "2. the padding mask is no a triu matrix, the broadcasted shape is `[B, 1, S]`, it's broadcasted in the `k/v` dim, since the some `k/v` should be masked out.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K/V Cache Optimization for inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If do not consider K/V cache, each iteration the GPT model generate one token, and that token is appended to the end of sequence, and the whole sequence is fed into the model again to generate the next token.\n",
    "\n",
    "This is not efficient for inference, since the K/V vector (output tensors of K/V gemm) of the context of each iteration is already computed in the previous iteration.\n",
    "\n",
    "GPT decoder has 2 stages:\n",
    "\n",
    "1. First stage the past K/V is empty, and the sequence of prompt tokens are fed in.\n",
    "    K, V of these context (prompt) are computed and outputed.\n",
    "    One token is generated as next token.\n",
    "\n",
    "2. Second stage. Repeat until the <end> token is genereted.\n",
    "    Past K/V + last token is fed in -> Current K/V and current token is generated.\n",
    "\n",
    "\n",
    "Attention with K/V cache ONNX graph looks like follows\n",
    "\n",
    "![attention with K/V cache](./media/multihead-attention-kv-cache-onnx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRT kernels of decoder layer with attention using K/V cache looks like follows:\n",
    "\n",
    "![decoder layer with K/V cache](./media/multihead-attention-trt-kernel-kv-cache.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the K/V slice concat reshape transpose are fused into one kernel. Whose fusion is even better than the multi-head attention without K/V cache.\n",
    "W/o cache, there are 13 kernels shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
