{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer blocks in Depth\n",
    "\n",
    "This blog target to analyze the transformer blocks in depth, from pytorch code to the kernel level implementation, and discuss the details of the potential optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention Layer\n",
    "\n",
    "### 1.1. Vanilla self-attention layer in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAtten(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=512) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qkv_proj = nn.Linear(dim, dim*3)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: [B, S, D]\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        # [B, S, S]\n",
    "        score = torch.matmul(q, k.transpose(-2, -1)) / (self.dim ** 0.5) \n",
    "        score = score.masked_fill(mask == 0, -1e3)\n",
    "        attn = score.softmax(dim=-1)\n",
    "        # [B, S, D]\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch implementation of the above layer requires 10 kernels, which are listed below:\n",
    "\n",
    "4 GEMM kernels.\n",
    "1 Softmax kernel.\n",
    "And some elementwise for the mask compute.\n",
    "\n",
    "![kernels](./media/attention-torch-kernel-trace.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnx graph\n",
    "\n",
    "![onnx graph of attention layer](./media/attention.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT kernels\n",
    "\n",
    "After the onnx lowered to TRT, some elementwise kernels and softmax kernels are fused together, there are only 7 kernels in total.\n",
    "![kernels](./media/attention-trt-kernel.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Analyze of the fusions:\n",
    "\n",
    "- Bias of first GEMM, and 3 slices (tensor.chunk) are fused together into _myl_bb0_3_AddSliSliSli\n",
    "    The shape compute is done in host, no kernels needed.\n",
    "    \n",
    "- Transpose of the QK^T gemm is fused with Matmul\n",
    "- Mask and softmax are fused together into _myl_bb0_2_*\n",
    "- The bias add after output projection gemm is not fused, which is not good.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0faca9c0a7487b0a8cab6682032d0e1bdce24ee994a5eef8cee33e52fa5fa0cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
