{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer blocks in Depth\n",
    "\n",
    "This blog target to analyze the transformer blocks in depth, from pytorch code to the kernel level implementation, and discuss the details of the potential optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention Layer\n",
    "\n",
    "### 1.1. Vanilla self-attention layer in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAtten(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=512) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qkv_proj = nn.Linear(dim, dim*3)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: [B, S, D]\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        # [B, S, S]\n",
    "        score = torch.matmul(q, k.transpose(-2, -1)) / (self.dim ** 0.5) \n",
    "        score = score.masked_fill(mask == 0, -1e3)\n",
    "        attn = score.softmax(dim=-1)\n",
    "        # [B, S, D]\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch implementation of the above layer requires 10 kernels, which are listed below:\n",
    "\n",
    "4 GEMM kernels.\n",
    "1 Softmax kernel.\n",
    "And some elementwise for the mask compute.\n",
    "\n",
    "![kernels](./media/attention-torch-kernel-trace.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnx graph\n",
    "\n",
    "![onnx graph of attention layer](./media/attention.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT kernels\n",
    "\n",
    "After the onnx lowered to TRT, some elementwise kernels and softmax kernels are fused together, there are only 7 kernels in total.\n",
    "![kernels](./media/attention-trt-kernel.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Analyze of the fusions:\n",
    "\n",
    "- Bias of first GEMM, and 3 slices (tensor.chunk) are fused together into _myl_bb0_3_AddSliSliSli\n",
    "    The shape compute is done in host, no kernels needed.\n",
    "    \n",
    "- Transpose of the QK^T gemm is fused with Matmul\n",
    "- Mask and softmax are fused together into _myl_bb0_2_*\n",
    "- The bias add after output projection gemm is not fused, which is not good.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadSelfAtten(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=512, head=8) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.head = head\n",
    "        self.qkv_proj = nn.Linear(dim, dim*3)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    # [B, S, D] -> [B, H, S, D/H]\n",
    "    def reshape(self, x):\n",
    "        return x.reshape(x.shape[0], -1, self.head, self.dim // self.head).transpose(1, 2)\n",
    "\n",
    "    def self_attn(self, q, k, v, mask):\n",
    "        # q, k, v: [B, H, S, D/H]\n",
    "        # out: [B, H, S, D/H]\n",
    "        score = torch.matmul(q, k.transpose(-2, -1)) / (self.dim ** 0.5) \n",
    "        score = score.masked_fill(mask == 0, -1e3)\n",
    "        attn = score.softmax(dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: [B, S, D]\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        batch = q.shape[0]\n",
    "        q, k, v = self.reshape(q), self.reshape(k), self.reshape(v)\n",
    "        out = self.self_attn(q, k, v, mask)\n",
    "        # merge heads: [B, H, S, D/H] -> [B, S, D]\n",
    "        out = out.transpose(1, 2).reshape(batch, -1, self.dim)\n",
    "\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnx graph of multi head attention\n",
    "![onnx graph of multi head attention](./media/multihead-attention-onnx.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare with the single head self-attention, multi-head attention has addtional reshape and transpose before the self-attention compute, which moves the head dimension as batch dim, and do attention, and then transpose->reshpae back, so the onnx graph is more complicated.\n",
    "\n",
    "Following 9 TRT kernels are launched for the multi-head attention:\n",
    "\n",
    "![multi-head attention kernels](./media/multihead-attention-trt-kernel.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "norm = nn.LayerNorm(512) \n",
    "x = norm(torch.rand(1, 32, 512))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pytorch, the layer norm is done by one kernel. The onnx graph looks like this\n",
    "![onnx graph of layer norm](./media/layer-norm-onnx.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Whole Transformer Decoder Block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![trt kernels of transformer decoder block](./media/decoder-attention-trt-kernel.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoel decoder block needs 13 kernels in TRT, addtional 4 kernels compared with the multi head attention.\n",
    "\n",
    "- 2 LayerNorm+Residual. LayerNorm and the residual are fused together, which is good. \n",
    "- 1 GELU kernel\n",
    "- 2 MLP gemm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0faca9c0a7487b0a8cab6682032d0e1bdce24ee994a5eef8cee33e52fa5fa0cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
